# 通过 API 调用
大模型的调用可以通过 `API` 实现。如果要更好的使用大模型我们需要有一个 AI 桌面/网页客户端，这里推荐使用 [ChatBox](https://chatboxai.app/zh)。

如果你已经获取到模型的 `API 密匙`及 `API 域名`，那么在设置中你可以轻而易举的调用 API，使用该 API 指向的大模型或者自选模型。当然，你也可以通过`Python、JavaScript、curl、Node.js`等多种方式通过 API 调用。

# 通过 Ollama 本地部署调用

## Ollama 的简介

`Ollama` 是一个开源项目，旨在简化机器学习模型的使用和部署，特别是大型语言模型（LLM）。

1. Ollama 提供了一个简单的命令行工具，使得下载、管理和使用机器学习模型变得更加方便。用户可以通过简单的命令来获取和运行不同的模型。
2. Ollama 允许用户在本地机器上运行模型，而无需依赖云服务。这意味着用户可以更好地控制数据隐私和安全性，同时可以在离线环境中使用模型。
3. Ollama 根据用户的硬件和需求，对模型进行了优化，以提高推理速度和效率。这使得即使在资源有限的设备上，也能获得良好的性能。
4. Ollama 支持多种流行的语言模型，用户可以根据自己的需求选择合适的模型进行使用。

## 如何使用 Ollama

1. 安装：https://ollama.com/
2. 下载模型：

```bash
ollama pull <model-name>
```

3. 命令行运行模型：

```bash
ollama run <model-name>
```

4. 开启Ollama serve

```bash
ollama serve <model-name>
```

## 如何使用 Ollama API

在成功下载模型并启用 ollama serve 之后，此时的大模型 API 域名即为`http://localhost:11434`，也就是本地的`11434`端口。我们进入该网址如果看到`Ollama is running`的字样，证明模型已经成功在本地运行了。此时回到 ChatBox 的设置页面，模型提供方选择`Ollama API`，选择模型后即可运行下载的大模型。这样，我们就通过 Ollama + ChatBox 实现了大模型的本地部署。

## 关于模型

一般情况下，我们的电脑并不支持运行例如`deepseek-r1`这样 671B 的巨无霸模型。一般情况下运行 1.5B 的模型是完全没有问题的，7B 左右的模型就因机器而异了。更多要点就不在此展开了。
